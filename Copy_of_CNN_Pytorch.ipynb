{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtv/ouNFD+r0+AFwhXULgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjayPvs/DL-project/blob/main/Copy_of_CNN_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_gbIT0IKS6b",
        "outputId": "e654160f-8505-404d-a788-8922e0f6f403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2) (1.3.0)\n",
            "Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 triton-2.2.0\n",
            "Collecting torchvision==0.17\n",
            "  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision==0.17) (12.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchvision==0.17) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->torchvision==0.17) (1.3.0)\n",
            "Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "Successfully installed torchvision-0.17.0\n",
            "Collecting matplotlib==3.5.2\n",
            "  Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.16.0)\n",
            "Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.12.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.2 which is incompatible.\n",
            "plotnine 0.12.4 requires matplotlib>=3.6.0, but you have matplotlib 3.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "8167c81b76a84de9a487c434dcbdd4ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 10459682.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1848158.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13702676.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3768650.60it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
            "epoch: 1 [320/60000 (1%)]\t training loss: 1.924133\n",
            "epoch: 1 [640/60000 (1%)]\t training loss: 1.313336\n",
            "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
            "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
            "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678430\n",
            "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477187\n",
            "epoch: 1 [2240/60000 (4%)]\t training loss: 0.529062\n",
            "epoch: 1 [2560/60000 (4%)]\t training loss: 0.468829\n",
            "epoch: 1 [2880/60000 (5%)]\t training loss: 0.242887\n",
            "epoch: 1 [3200/60000 (5%)]\t training loss: 0.519633\n",
            "epoch: 1 [3520/60000 (6%)]\t training loss: 0.262872\n",
            "epoch: 1 [3840/60000 (6%)]\t training loss: 0.467795\n",
            "epoch: 1 [4160/60000 (7%)]\t training loss: 0.417298\n",
            "epoch: 1 [4480/60000 (7%)]\t training loss: 0.316030\n",
            "epoch: 1 [4800/60000 (8%)]\t training loss: 0.498437\n",
            "epoch: 1 [5120/60000 (9%)]\t training loss: 0.157880\n",
            "epoch: 1 [5440/60000 (9%)]\t training loss: 0.364403\n",
            "epoch: 1 [5760/60000 (10%)]\t training loss: 0.085984\n",
            "epoch: 1 [6080/60000 (10%)]\t training loss: 0.177615\n",
            "epoch: 1 [6400/60000 (11%)]\t training loss: 0.278263\n",
            "epoch: 1 [6720/60000 (11%)]\t training loss: 0.082559\n",
            "epoch: 1 [7040/60000 (12%)]\t training loss: 0.351368\n",
            "epoch: 1 [7360/60000 (12%)]\t training loss: 0.040539\n",
            "epoch: 1 [7680/60000 (13%)]\t training loss: 0.367039\n",
            "epoch: 1 [8000/60000 (13%)]\t training loss: 0.219277\n",
            "epoch: 1 [8320/60000 (14%)]\t training loss: 0.329156\n",
            "epoch: 1 [8640/60000 (14%)]\t training loss: 0.214590\n",
            "epoch: 1 [8960/60000 (15%)]\t training loss: 0.057667\n",
            "epoch: 1 [9280/60000 (15%)]\t training loss: 0.345586\n",
            "epoch: 1 [9600/60000 (16%)]\t training loss: 0.205804\n",
            "epoch: 1 [9920/60000 (17%)]\t training loss: 0.209767\n",
            "epoch: 1 [10240/60000 (17%)]\t training loss: 0.209993\n",
            "epoch: 1 [10560/60000 (18%)]\t training loss: 0.403128\n",
            "epoch: 1 [10880/60000 (18%)]\t training loss: 0.071791\n",
            "epoch: 1 [11200/60000 (19%)]\t training loss: 0.069998\n",
            "epoch: 1 [11520/60000 (19%)]\t training loss: 0.231422\n",
            "epoch: 1 [11840/60000 (20%)]\t training loss: 0.072475\n",
            "epoch: 1 [12160/60000 (20%)]\t training loss: 0.409647\n",
            "epoch: 1 [12480/60000 (21%)]\t training loss: 0.021981\n",
            "epoch: 1 [12800/60000 (21%)]\t training loss: 0.093394\n",
            "epoch: 1 [13120/60000 (22%)]\t training loss: 0.085837\n",
            "epoch: 1 [13440/60000 (22%)]\t training loss: 0.161934\n",
            "epoch: 1 [13760/60000 (23%)]\t training loss: 0.065058\n",
            "epoch: 1 [14080/60000 (23%)]\t training loss: 0.046802\n",
            "epoch: 1 [14400/60000 (24%)]\t training loss: 0.083767\n",
            "epoch: 1 [14720/60000 (25%)]\t training loss: 0.167542\n",
            "epoch: 1 [15040/60000 (25%)]\t training loss: 0.345123\n",
            "epoch: 1 [15360/60000 (26%)]\t training loss: 0.091196\n",
            "epoch: 1 [15680/60000 (26%)]\t training loss: 0.108777\n",
            "epoch: 1 [16000/60000 (27%)]\t training loss: 0.075832\n",
            "epoch: 1 [16320/60000 (27%)]\t training loss: 0.171944\n",
            "epoch: 1 [16640/60000 (28%)]\t training loss: 0.029883\n",
            "epoch: 1 [16960/60000 (28%)]\t training loss: 0.333812\n",
            "epoch: 1 [17280/60000 (29%)]\t training loss: 0.060629\n",
            "epoch: 1 [17600/60000 (29%)]\t training loss: 0.028994\n",
            "epoch: 1 [17920/60000 (30%)]\t training loss: 0.040753\n",
            "epoch: 1 [18240/60000 (30%)]\t training loss: 0.019892\n",
            "epoch: 1 [18560/60000 (31%)]\t training loss: 0.057777\n",
            "epoch: 1 [18880/60000 (31%)]\t training loss: 0.236884\n",
            "epoch: 1 [19200/60000 (32%)]\t training loss: 0.088455\n",
            "epoch: 1 [19520/60000 (33%)]\t training loss: 0.067441\n",
            "epoch: 1 [19840/60000 (33%)]\t training loss: 0.063453\n",
            "epoch: 1 [20160/60000 (34%)]\t training loss: 0.107841\n",
            "epoch: 1 [20480/60000 (34%)]\t training loss: 0.060509\n",
            "epoch: 1 [20800/60000 (35%)]\t training loss: 0.205607\n",
            "epoch: 1 [21120/60000 (35%)]\t training loss: 0.280535\n",
            "epoch: 1 [21440/60000 (36%)]\t training loss: 0.262102\n",
            "epoch: 1 [21760/60000 (36%)]\t training loss: 0.109629\n",
            "epoch: 1 [22080/60000 (37%)]\t training loss: 0.096564\n",
            "epoch: 1 [22400/60000 (37%)]\t training loss: 0.134667\n",
            "epoch: 1 [22720/60000 (38%)]\t training loss: 0.265525\n",
            "epoch: 1 [23040/60000 (38%)]\t training loss: 0.076615\n",
            "epoch: 1 [23360/60000 (39%)]\t training loss: 0.125220\n",
            "epoch: 1 [23680/60000 (39%)]\t training loss: 0.251269\n",
            "epoch: 1 [24000/60000 (40%)]\t training loss: 0.215296\n",
            "epoch: 1 [24320/60000 (41%)]\t training loss: 0.065185\n",
            "epoch: 1 [24640/60000 (41%)]\t training loss: 0.140391\n",
            "epoch: 1 [24960/60000 (42%)]\t training loss: 0.027044\n",
            "epoch: 1 [25280/60000 (42%)]\t training loss: 0.024160\n",
            "epoch: 1 [25600/60000 (43%)]\t training loss: 0.570725\n",
            "epoch: 1 [25920/60000 (43%)]\t training loss: 0.460728\n",
            "epoch: 1 [26240/60000 (44%)]\t training loss: 0.129823\n",
            "epoch: 1 [26560/60000 (44%)]\t training loss: 0.040875\n",
            "epoch: 1 [26880/60000 (45%)]\t training loss: 0.150539\n",
            "epoch: 1 [27200/60000 (45%)]\t training loss: 0.018501\n",
            "epoch: 1 [27520/60000 (46%)]\t training loss: 0.040860\n",
            "epoch: 1 [27840/60000 (46%)]\t training loss: 0.136634\n",
            "epoch: 1 [28160/60000 (47%)]\t training loss: 0.048300\n",
            "epoch: 1 [28480/60000 (47%)]\t training loss: 0.392610\n",
            "epoch: 1 [28800/60000 (48%)]\t training loss: 0.119276\n",
            "epoch: 1 [29120/60000 (49%)]\t training loss: 0.198008\n",
            "epoch: 1 [29440/60000 (49%)]\t training loss: 0.014311\n",
            "epoch: 1 [29760/60000 (50%)]\t training loss: 0.053688\n",
            "epoch: 1 [30080/60000 (50%)]\t training loss: 0.541627\n",
            "epoch: 1 [30400/60000 (51%)]\t training loss: 0.127081\n",
            "epoch: 1 [30720/60000 (51%)]\t training loss: 0.230976\n",
            "epoch: 1 [31040/60000 (52%)]\t training loss: 0.080813\n",
            "epoch: 1 [31360/60000 (52%)]\t training loss: 0.070519\n",
            "epoch: 1 [31680/60000 (53%)]\t training loss: 0.026341\n",
            "epoch: 1 [32000/60000 (53%)]\t training loss: 0.011774\n",
            "epoch: 1 [32320/60000 (54%)]\t training loss: 0.110159\n",
            "epoch: 1 [32640/60000 (54%)]\t training loss: 0.095929\n",
            "epoch: 1 [32960/60000 (55%)]\t training loss: 0.042572\n",
            "epoch: 1 [33280/60000 (55%)]\t training loss: 0.260714\n",
            "epoch: 1 [33600/60000 (56%)]\t training loss: 0.192777\n",
            "epoch: 1 [33920/60000 (57%)]\t training loss: 0.019657\n",
            "epoch: 1 [34240/60000 (57%)]\t training loss: 0.024119\n",
            "epoch: 1 [34560/60000 (58%)]\t training loss: 0.002996\n",
            "epoch: 1 [34880/60000 (58%)]\t training loss: 0.175705\n",
            "epoch: 1 [35200/60000 (59%)]\t training loss: 0.279498\n",
            "epoch: 1 [35520/60000 (59%)]\t training loss: 0.133032\n",
            "epoch: 1 [35840/60000 (60%)]\t training loss: 0.012259\n",
            "epoch: 1 [36160/60000 (60%)]\t training loss: 0.141879\n",
            "epoch: 1 [36480/60000 (61%)]\t training loss: 0.105978\n",
            "epoch: 1 [36800/60000 (61%)]\t training loss: 0.136131\n",
            "epoch: 1 [37120/60000 (62%)]\t training loss: 0.046783\n",
            "epoch: 1 [37440/60000 (62%)]\t training loss: 0.229350\n",
            "epoch: 1 [37760/60000 (63%)]\t training loss: 0.023725\n",
            "epoch: 1 [38080/60000 (63%)]\t training loss: 0.125504\n",
            "epoch: 1 [38400/60000 (64%)]\t training loss: 0.044750\n",
            "epoch: 1 [38720/60000 (65%)]\t training loss: 0.422863\n",
            "epoch: 1 [39040/60000 (65%)]\t training loss: 0.047209\n",
            "epoch: 1 [39360/60000 (66%)]\t training loss: 0.201553\n",
            "epoch: 1 [39680/60000 (66%)]\t training loss: 0.020029\n",
            "epoch: 1 [40000/60000 (67%)]\t training loss: 0.153952\n",
            "epoch: 1 [40320/60000 (67%)]\t training loss: 0.078929\n",
            "epoch: 1 [40640/60000 (68%)]\t training loss: 0.169443\n",
            "epoch: 1 [40960/60000 (68%)]\t training loss: 0.198745\n",
            "epoch: 1 [41280/60000 (69%)]\t training loss: 0.204217\n",
            "epoch: 1 [41600/60000 (69%)]\t training loss: 0.136150\n",
            "epoch: 1 [41920/60000 (70%)]\t training loss: 0.039529\n",
            "epoch: 1 [42240/60000 (70%)]\t training loss: 0.011084\n",
            "epoch: 1 [42560/60000 (71%)]\t training loss: 0.041894\n",
            "epoch: 1 [42880/60000 (71%)]\t training loss: 0.121699\n",
            "epoch: 1 [43200/60000 (72%)]\t training loss: 0.057140\n",
            "epoch: 1 [43520/60000 (73%)]\t training loss: 0.245478\n",
            "epoch: 1 [43840/60000 (73%)]\t training loss: 0.057972\n",
            "epoch: 1 [44160/60000 (74%)]\t training loss: 0.153300\n",
            "epoch: 1 [44480/60000 (74%)]\t training loss: 0.068017\n",
            "epoch: 1 [44800/60000 (75%)]\t training loss: 0.214221\n",
            "epoch: 1 [45120/60000 (75%)]\t training loss: 0.019666\n",
            "epoch: 1 [45440/60000 (76%)]\t training loss: 0.183320\n",
            "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004745\n",
            "epoch: 1 [46080/60000 (77%)]\t training loss: 0.230460\n",
            "epoch: 1 [46400/60000 (77%)]\t training loss: 0.047609\n",
            "epoch: 1 [46720/60000 (78%)]\t training loss: 0.104887\n",
            "epoch: 1 [47040/60000 (78%)]\t training loss: 0.056543\n",
            "epoch: 1 [47360/60000 (79%)]\t training loss: 0.026343\n",
            "epoch: 1 [47680/60000 (79%)]\t training loss: 0.208909\n",
            "epoch: 1 [48000/60000 (80%)]\t training loss: 0.173453\n",
            "epoch: 1 [48320/60000 (81%)]\t training loss: 0.171665\n",
            "epoch: 1 [48640/60000 (81%)]\t training loss: 0.011326\n",
            "epoch: 1 [48960/60000 (82%)]\t training loss: 0.078636\n",
            "epoch: 1 [49280/60000 (82%)]\t training loss: 0.175529\n",
            "epoch: 1 [49600/60000 (83%)]\t training loss: 0.225299\n",
            "epoch: 1 [49920/60000 (83%)]\t training loss: 0.166357\n",
            "epoch: 1 [50240/60000 (84%)]\t training loss: 0.066043\n",
            "epoch: 1 [50560/60000 (84%)]\t training loss: 0.003922\n",
            "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004356\n",
            "epoch: 1 [51200/60000 (85%)]\t training loss: 0.197849\n",
            "epoch: 1 [51520/60000 (86%)]\t training loss: 0.025213\n",
            "epoch: 1 [51840/60000 (86%)]\t training loss: 0.018620\n",
            "epoch: 1 [52160/60000 (87%)]\t training loss: 0.012329\n",
            "epoch: 1 [52480/60000 (87%)]\t training loss: 0.009802\n",
            "epoch: 1 [52800/60000 (88%)]\t training loss: 0.033116\n",
            "epoch: 1 [53120/60000 (89%)]\t training loss: 0.028992\n",
            "epoch: 1 [53440/60000 (89%)]\t training loss: 0.013409\n",
            "epoch: 1 [53760/60000 (90%)]\t training loss: 0.037544\n",
            "epoch: 1 [54080/60000 (90%)]\t training loss: 0.011171\n",
            "epoch: 1 [54400/60000 (91%)]\t training loss: 0.117641\n",
            "epoch: 1 [54720/60000 (91%)]\t training loss: 0.013717\n",
            "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001378\n",
            "epoch: 1 [55360/60000 (92%)]\t training loss: 0.045877\n",
            "epoch: 1 [55680/60000 (93%)]\t training loss: 0.052783\n",
            "epoch: 1 [56000/60000 (93%)]\t training loss: 0.048510\n",
            "epoch: 1 [56320/60000 (94%)]\t training loss: 0.142568\n",
            "epoch: 1 [56640/60000 (94%)]\t training loss: 0.053807\n",
            "epoch: 1 [56960/60000 (95%)]\t training loss: 0.029687\n",
            "epoch: 1 [57280/60000 (95%)]\t training loss: 0.087261\n",
            "epoch: 1 [57600/60000 (96%)]\t training loss: 0.003207\n",
            "epoch: 1 [57920/60000 (97%)]\t training loss: 0.095157\n",
            "epoch: 1 [58240/60000 (97%)]\t training loss: 0.015331\n",
            "epoch: 1 [58560/60000 (98%)]\t training loss: 0.006808\n",
            "epoch: 1 [58880/60000 (98%)]\t training loss: 0.091058\n",
            "epoch: 1 [59200/60000 (99%)]\t training loss: 0.005085\n",
            "epoch: 1 [59520/60000 (99%)]\t training loss: 0.031503\n",
            "epoch: 1 [59840/60000 (100%)]\t training loss: 0.035041\n",
            "\n",
            "Test dataset: Overall Loss: 0.0481, Overall Accuracy: 9839/10000 (98%)\n",
            "\n",
            "epoch: 2 [0/60000 (0%)]\t training loss: 0.075432\n",
            "epoch: 2 [320/60000 (1%)]\t training loss: 0.046304\n",
            "epoch: 2 [640/60000 (1%)]\t training loss: 0.123041\n",
            "epoch: 2 [960/60000 (2%)]\t training loss: 0.068879\n",
            "epoch: 2 [1280/60000 (2%)]\t training loss: 0.054027\n",
            "epoch: 2 [1600/60000 (3%)]\t training loss: 0.002742\n",
            "epoch: 2 [1920/60000 (3%)]\t training loss: 0.076803\n",
            "epoch: 2 [2240/60000 (4%)]\t training loss: 0.025736\n",
            "epoch: 2 [2560/60000 (4%)]\t training loss: 0.112260\n",
            "epoch: 2 [2880/60000 (5%)]\t training loss: 0.008886\n",
            "epoch: 2 [3200/60000 (5%)]\t training loss: 0.077838\n",
            "epoch: 2 [3520/60000 (6%)]\t training loss: 0.015863\n",
            "epoch: 2 [3840/60000 (6%)]\t training loss: 0.098132\n",
            "epoch: 2 [4160/60000 (7%)]\t training loss: 0.021622\n",
            "epoch: 2 [4480/60000 (7%)]\t training loss: 0.020830\n",
            "epoch: 2 [4800/60000 (8%)]\t training loss: 0.017926\n",
            "epoch: 2 [5120/60000 (9%)]\t training loss: 0.185184\n",
            "epoch: 2 [5440/60000 (9%)]\t training loss: 0.149663\n",
            "epoch: 2 [5760/60000 (10%)]\t training loss: 0.089693\n",
            "epoch: 2 [6080/60000 (10%)]\t training loss: 0.070531\n",
            "epoch: 2 [6400/60000 (11%)]\t training loss: 0.034534\n",
            "epoch: 2 [6720/60000 (11%)]\t training loss: 0.049013\n",
            "epoch: 2 [7040/60000 (12%)]\t training loss: 0.012918\n",
            "epoch: 2 [7360/60000 (12%)]\t training loss: 0.101116\n",
            "epoch: 2 [7680/60000 (13%)]\t training loss: 0.016830\n",
            "epoch: 2 [8000/60000 (13%)]\t training loss: 0.235459\n",
            "epoch: 2 [8320/60000 (14%)]\t training loss: 0.088455\n",
            "epoch: 2 [8640/60000 (14%)]\t training loss: 0.020136\n",
            "epoch: 2 [8960/60000 (15%)]\t training loss: 0.030928\n",
            "epoch: 2 [9280/60000 (15%)]\t training loss: 0.002639\n",
            "epoch: 2 [9600/60000 (16%)]\t training loss: 0.062741\n",
            "epoch: 2 [9920/60000 (17%)]\t training loss: 0.003497\n",
            "epoch: 2 [10240/60000 (17%)]\t training loss: 0.007688\n",
            "epoch: 2 [10560/60000 (18%)]\t training loss: 0.001969\n",
            "epoch: 2 [10880/60000 (18%)]\t training loss: 0.087715\n",
            "epoch: 2 [11200/60000 (19%)]\t training loss: 0.021731\n",
            "epoch: 2 [11520/60000 (19%)]\t training loss: 0.017555\n",
            "epoch: 2 [11840/60000 (20%)]\t training loss: 0.104052\n",
            "epoch: 2 [12160/60000 (20%)]\t training loss: 0.068002\n",
            "epoch: 2 [12480/60000 (21%)]\t training loss: 0.053844\n",
            "epoch: 2 [12800/60000 (21%)]\t training loss: 0.006709\n",
            "epoch: 2 [13120/60000 (22%)]\t training loss: 0.190556\n",
            "epoch: 2 [13440/60000 (22%)]\t training loss: 0.033643\n",
            "epoch: 2 [13760/60000 (23%)]\t training loss: 0.115461\n",
            "epoch: 2 [14080/60000 (23%)]\t training loss: 0.042927\n",
            "epoch: 2 [14400/60000 (24%)]\t training loss: 0.034154\n",
            "epoch: 2 [14720/60000 (25%)]\t training loss: 0.007279\n",
            "epoch: 2 [15040/60000 (25%)]\t training loss: 0.002353\n",
            "epoch: 2 [15360/60000 (26%)]\t training loss: 0.073108\n",
            "epoch: 2 [15680/60000 (26%)]\t training loss: 0.039223\n",
            "epoch: 2 [16000/60000 (27%)]\t training loss: 0.090295\n",
            "epoch: 2 [16320/60000 (27%)]\t training loss: 0.156239\n",
            "epoch: 2 [16640/60000 (28%)]\t training loss: 0.007567\n",
            "epoch: 2 [16960/60000 (28%)]\t training loss: 0.370483\n",
            "epoch: 2 [17280/60000 (29%)]\t training loss: 0.052054\n",
            "epoch: 2 [17600/60000 (29%)]\t training loss: 0.082525\n",
            "epoch: 2 [17920/60000 (30%)]\t training loss: 0.091500\n",
            "epoch: 2 [18240/60000 (30%)]\t training loss: 0.081759\n",
            "epoch: 2 [18560/60000 (31%)]\t training loss: 0.002456\n",
            "epoch: 2 [18880/60000 (31%)]\t training loss: 0.086008\n",
            "epoch: 2 [19200/60000 (32%)]\t training loss: 0.111545\n",
            "epoch: 2 [19520/60000 (33%)]\t training loss: 0.012260\n",
            "epoch: 2 [19840/60000 (33%)]\t training loss: 0.061752\n",
            "epoch: 2 [20160/60000 (34%)]\t training loss: 0.014947\n",
            "epoch: 2 [20480/60000 (34%)]\t training loss: 0.107861\n",
            "epoch: 2 [20800/60000 (35%)]\t training loss: 0.018007\n",
            "epoch: 2 [21120/60000 (35%)]\t training loss: 0.071745\n",
            "epoch: 2 [21440/60000 (36%)]\t training loss: 0.047280\n",
            "epoch: 2 [21760/60000 (36%)]\t training loss: 0.011887\n",
            "epoch: 2 [22080/60000 (37%)]\t training loss: 0.018028\n",
            "epoch: 2 [22400/60000 (37%)]\t training loss: 0.002997\n",
            "epoch: 2 [22720/60000 (38%)]\t training loss: 0.007426\n",
            "epoch: 2 [23040/60000 (38%)]\t training loss: 0.076983\n",
            "epoch: 2 [23360/60000 (39%)]\t training loss: 0.119280\n",
            "epoch: 2 [23680/60000 (39%)]\t training loss: 0.178616\n",
            "epoch: 2 [24000/60000 (40%)]\t training loss: 0.024068\n",
            "epoch: 2 [24320/60000 (41%)]\t training loss: 0.024153\n",
            "epoch: 2 [24640/60000 (41%)]\t training loss: 0.096584\n",
            "epoch: 2 [24960/60000 (42%)]\t training loss: 0.044256\n",
            "epoch: 2 [25280/60000 (42%)]\t training loss: 0.080778\n",
            "epoch: 2 [25600/60000 (43%)]\t training loss: 0.094158\n",
            "epoch: 2 [25920/60000 (43%)]\t training loss: 0.012723\n",
            "epoch: 2 [26240/60000 (44%)]\t training loss: 0.001965\n",
            "epoch: 2 [26560/60000 (44%)]\t training loss: 0.008722\n",
            "epoch: 2 [26880/60000 (45%)]\t training loss: 0.049342\n",
            "epoch: 2 [27200/60000 (45%)]\t training loss: 0.437475\n",
            "epoch: 2 [27520/60000 (46%)]\t training loss: 0.093486\n",
            "epoch: 2 [27840/60000 (46%)]\t training loss: 0.005935\n",
            "epoch: 2 [28160/60000 (47%)]\t training loss: 0.035793\n",
            "epoch: 2 [28480/60000 (47%)]\t training loss: 0.070024\n",
            "epoch: 2 [28800/60000 (48%)]\t training loss: 0.008796\n",
            "epoch: 2 [29120/60000 (49%)]\t training loss: 0.005460\n",
            "epoch: 2 [29440/60000 (49%)]\t training loss: 0.075453\n",
            "epoch: 2 [29760/60000 (50%)]\t training loss: 0.082600\n",
            "epoch: 2 [30080/60000 (50%)]\t training loss: 0.025095\n",
            "epoch: 2 [30400/60000 (51%)]\t training loss: 0.008946\n",
            "epoch: 2 [30720/60000 (51%)]\t training loss: 0.017521\n",
            "epoch: 2 [31040/60000 (52%)]\t training loss: 0.144723\n",
            "epoch: 2 [31360/60000 (52%)]\t training loss: 0.040355\n",
            "epoch: 2 [31680/60000 (53%)]\t training loss: 0.009017\n",
            "epoch: 2 [32000/60000 (53%)]\t training loss: 0.003635\n",
            "epoch: 2 [32320/60000 (54%)]\t training loss: 0.007101\n",
            "epoch: 2 [32640/60000 (54%)]\t training loss: 0.021724\n",
            "epoch: 2 [32960/60000 (55%)]\t training loss: 0.000807\n",
            "epoch: 2 [33280/60000 (55%)]\t training loss: 0.173847\n",
            "epoch: 2 [33600/60000 (56%)]\t training loss: 0.010778\n",
            "epoch: 2 [33920/60000 (57%)]\t training loss: 0.013701\n",
            "epoch: 2 [34240/60000 (57%)]\t training loss: 0.399148\n",
            "epoch: 2 [34560/60000 (58%)]\t training loss: 0.005925\n",
            "epoch: 2 [34880/60000 (58%)]\t training loss: 0.115400\n",
            "epoch: 2 [35200/60000 (59%)]\t training loss: 0.012402\n",
            "epoch: 2 [35520/60000 (59%)]\t training loss: 0.039522\n",
            "epoch: 2 [35840/60000 (60%)]\t training loss: 0.003374\n",
            "epoch: 2 [36160/60000 (60%)]\t training loss: 0.032543\n",
            "epoch: 2 [36480/60000 (61%)]\t training loss: 0.080096\n",
            "epoch: 2 [36800/60000 (61%)]\t training loss: 0.002783\n",
            "epoch: 2 [37120/60000 (62%)]\t training loss: 0.456542\n",
            "epoch: 2 [37440/60000 (62%)]\t training loss: 0.004737\n",
            "epoch: 2 [37760/60000 (63%)]\t training loss: 0.076467\n",
            "epoch: 2 [38080/60000 (63%)]\t training loss: 0.019500\n",
            "epoch: 2 [38400/60000 (64%)]\t training loss: 0.035769\n",
            "epoch: 2 [38720/60000 (65%)]\t training loss: 0.074662\n",
            "epoch: 2 [39040/60000 (65%)]\t training loss: 0.034344\n",
            "epoch: 2 [39360/60000 (66%)]\t training loss: 0.004834\n",
            "epoch: 2 [39680/60000 (66%)]\t training loss: 0.155939\n",
            "epoch: 2 [40000/60000 (67%)]\t training loss: 0.013532\n",
            "epoch: 2 [40320/60000 (67%)]\t training loss: 0.274997\n",
            "epoch: 2 [40640/60000 (68%)]\t training loss: 0.598912\n",
            "epoch: 2 [40960/60000 (68%)]\t training loss: 0.069245\n",
            "epoch: 2 [41280/60000 (69%)]\t training loss: 0.055428\n",
            "epoch: 2 [41600/60000 (69%)]\t training loss: 0.175866\n",
            "epoch: 2 [41920/60000 (70%)]\t training loss: 0.005689\n",
            "epoch: 2 [42240/60000 (70%)]\t training loss: 0.010322\n",
            "epoch: 2 [42560/60000 (71%)]\t training loss: 0.049259\n",
            "epoch: 2 [42880/60000 (71%)]\t training loss: 0.006694\n",
            "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002140\n",
            "epoch: 2 [43520/60000 (73%)]\t training loss: 0.077839\n",
            "epoch: 2 [43840/60000 (73%)]\t training loss: 0.263577\n",
            "epoch: 2 [44160/60000 (74%)]\t training loss: 0.123324\n",
            "epoch: 2 [44480/60000 (74%)]\t training loss: 0.026996\n",
            "epoch: 2 [44800/60000 (75%)]\t training loss: 0.003256\n",
            "epoch: 2 [45120/60000 (75%)]\t training loss: 0.062740\n",
            "epoch: 2 [45440/60000 (76%)]\t training loss: 0.031622\n",
            "epoch: 2 [45760/60000 (76%)]\t training loss: 0.036767\n",
            "epoch: 2 [46080/60000 (77%)]\t training loss: 0.099838\n",
            "epoch: 2 [46400/60000 (77%)]\t training loss: 0.026845\n",
            "epoch: 2 [46720/60000 (78%)]\t training loss: 0.220059\n",
            "epoch: 2 [47040/60000 (78%)]\t training loss: 0.025425\n",
            "epoch: 2 [47360/60000 (79%)]\t training loss: 0.046500\n",
            "epoch: 2 [47680/60000 (79%)]\t training loss: 0.023531\n",
            "epoch: 2 [48000/60000 (80%)]\t training loss: 0.090858\n",
            "epoch: 2 [48320/60000 (81%)]\t training loss: 0.197350\n",
            "epoch: 2 [48640/60000 (81%)]\t training loss: 0.030297\n",
            "epoch: 2 [48960/60000 (82%)]\t training loss: 0.005242\n",
            "epoch: 2 [49280/60000 (82%)]\t training loss: 0.002309\n",
            "epoch: 2 [49600/60000 (83%)]\t training loss: 0.020609\n",
            "epoch: 2 [49920/60000 (83%)]\t training loss: 0.100927\n",
            "epoch: 2 [50240/60000 (84%)]\t training loss: 0.015600\n",
            "epoch: 2 [50560/60000 (84%)]\t training loss: 0.004015\n",
            "epoch: 2 [50880/60000 (85%)]\t training loss: 0.003906\n",
            "epoch: 2 [51200/60000 (85%)]\t training loss: 0.044343\n",
            "epoch: 2 [51520/60000 (86%)]\t training loss: 0.221729\n",
            "epoch: 2 [51840/60000 (86%)]\t training loss: 0.005866\n",
            "epoch: 2 [52160/60000 (87%)]\t training loss: 0.003225\n",
            "epoch: 2 [52480/60000 (87%)]\t training loss: 0.128596\n",
            "epoch: 2 [52800/60000 (88%)]\t training loss: 0.041003\n",
            "epoch: 2 [53120/60000 (89%)]\t training loss: 0.004447\n",
            "epoch: 2 [53440/60000 (89%)]\t training loss: 0.125383\n",
            "epoch: 2 [53760/60000 (90%)]\t training loss: 0.191748\n",
            "epoch: 2 [54080/60000 (90%)]\t training loss: 0.002825\n",
            "epoch: 2 [54400/60000 (91%)]\t training loss: 0.376830\n",
            "epoch: 2 [54720/60000 (91%)]\t training loss: 0.116594\n",
            "epoch: 2 [55040/60000 (92%)]\t training loss: 0.034064\n",
            "epoch: 2 [55360/60000 (92%)]\t training loss: 0.066723\n",
            "epoch: 2 [55680/60000 (93%)]\t training loss: 0.009449\n",
            "epoch: 2 [56000/60000 (93%)]\t training loss: 0.019677\n",
            "epoch: 2 [56320/60000 (94%)]\t training loss: 0.001398\n",
            "epoch: 2 [56640/60000 (94%)]\t training loss: 0.050175\n",
            "epoch: 2 [56960/60000 (95%)]\t training loss: 0.012732\n",
            "epoch: 2 [57280/60000 (95%)]\t training loss: 0.014090\n",
            "epoch: 2 [57600/60000 (96%)]\t training loss: 0.065312\n",
            "epoch: 2 [57920/60000 (97%)]\t training loss: 0.133847\n",
            "epoch: 2 [58240/60000 (97%)]\t training loss: 0.051631\n",
            "epoch: 2 [58560/60000 (98%)]\t training loss: 0.003867\n",
            "epoch: 2 [58880/60000 (98%)]\t training loss: 0.003583\n",
            "epoch: 2 [59200/60000 (99%)]\t training loss: 0.010058\n",
            "epoch: 2 [59520/60000 (99%)]\t training loss: 0.014423\n",
            "epoch: 2 [59840/60000 (100%)]\t training loss: 0.007865\n",
            "\n",
            "Test dataset: Overall Loss: 0.0406, Overall Accuracy: 9863/10000 (99%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install torch==2.2\n",
        "pip install torchvision==0.17\n",
        "pip install matplotlib==3.5.2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.dp1 = nn.Dropout2d(0.10)\n",
        "        self.dp2 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.cn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dp1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dp2(x)\n",
        "        x = self.fc2(x)\n",
        "        op = F.log_softmax(x, dim=1)\n",
        "        return op\n",
        "\n",
        "def train(model, device, train_dataloader, optim, epoch):\n",
        "    model.train()\n",
        "    for b_i, (X, y) in enumerate(train_dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optim.zero_grad()\n",
        "        pred_prob = model(X)\n",
        "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if b_i % 10 == 0:\n",
        "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
        "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
        "                100. * b_i / len(train_dataloader), loss.item()))\n",
        "\n",
        "def test(model, device, test_dataloader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    success = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred_prob = model(X)\n",
        "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
        "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
        "            success += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    loss /= len(test_dataloader.dataset)\n",
        "\n",
        "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        loss, success, len(test_dataloader.dataset),\n",
        "        100. * success / len(test_dataloader.dataset)))\n",
        "\n",
        "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
        "    batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))\n",
        "                   ])),\n",
        "    batch_size=500, shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = ConvNet()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
        "\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_dataloader, optimizer, epoch)\n",
        "    test(model, device, test_dataloader)\n",
        "\n"
      ]
    }
  ]
}